//! Worker Pool Management (Performance Optimized)
//!
//! Features:
//! 1. Work-stealing fallback strategy.
//! 2. Bounded channel capacity for pipeline handling.
//! 3. Batch-ready architecture for HTTP pipelining.
//! 4. Zero-copy / deferred cloning where possible.

use bytes::Bytes;
use crossbeam::channel::{bounded, Sender, TrySendError};
use std::sync::atomic::{AtomicUsize, Ordering};
use std::thread;
use tokio::sync::mpsc;
use tokio::sync::oneshot;
use smallvec::SmallVec;

use crate::extensions::{self, AsyncOpRequest, TitanRuntime, WorkerAsyncResult};

pub struct RuntimeManager {
    request_txs: Vec<Sender<WorkerCommand>>,
    round_robin_counter: AtomicUsize,
    num_workers: usize,
    _workers: Vec<thread::JoinHandle<()>>,
}

pub enum WorkerCommand {
    Request(RequestTask),
    Resume {
        drift_id: u32,
        result: WorkerAsyncResult,
    },
}

#[allow(dead_code)]
pub struct RequestTask {
    pub action_name: String,
    pub body: Option<Bytes>,
    pub method: String,
    pub path: String,
    pub headers: SmallVec<[(String, String); 8]>,
    pub params: SmallVec<[(String, String); 4]>,
    pub query: SmallVec<[(String, String); 4]>,
    pub response_tx: oneshot::Sender<WorkerResult>,
}

pub struct WorkerResult {
    pub json: serde_json::Value,
    pub timings: Vec<(String, f64)>,
}

impl RuntimeManager {
    pub fn new(
        project_root: std::path::PathBuf,
        num_threads: usize,
        stack_size: usize,
    ) -> Self {
        let (async_tx, mut async_rx) = mpsc::channel::<AsyncOpRequest>(2048);
        let tokio_handle = tokio::runtime::Handle::current();

        // Spawn Tokio Async Handler (for drift operations)
        tokio_handle.spawn(async move {
            while let Some(req) = async_rx.recv().await {
                let drift_id = req.drift_id;
                let respond_tx = req.respond_tx;
                tokio::spawn(async move {
                    let start = std::time::Instant::now();
                    let result = extensions::builtin::run_async_operation(req.op).await;
                    let duration_ms = start.elapsed().as_secs_f64() * 1000.0;
                    let _ = respond_tx.send(WorkerAsyncResult {
                        drift_id,
                        result,
                        duration_ms,
                    });
                });
            }
        });

        // Create worker channels
        let channel_capacity = 256;
        let mut workers = Vec::with_capacity(num_threads);

        let mut channels: Vec<(Sender<WorkerCommand>, crossbeam::channel::Receiver<WorkerCommand>)> =
            Vec::with_capacity(num_threads);

        for _ in 0..num_threads {
            let (tx, rx) = bounded(channel_capacity);
            channels.push((tx, rx));
        }

        let mut final_txs: Vec<Sender<WorkerCommand>> = Vec::with_capacity(num_threads);
        for (tx, _) in &channels {
            final_txs.push(tx.clone());
        }

        // Spawn Worker Threads
        for (i, (tx, rx)) in channels.into_iter().enumerate() {
            let my_tx = tx.clone();
            let root = project_root.clone();
            let handle = tokio_handle.clone();
            let async_tx = async_tx.clone();

            let handle = thread::Builder::new()
                .name(format!("titan-worker-{}", i))
                .stack_size(stack_size)
                .spawn(move || {
                    let mut rt = extensions::init_runtime_worker(
                        i,
                        root,
                        my_tx,
                        handle,
                        async_tx,
                        stack_size,
                    );
                    rt.bind_to_isolate();

                    loop {
                        match rx.recv() {
                            Ok(cmd) => match cmd {
                                WorkerCommand::Request(task) => {
                                    handle_new_request(task, &mut rt);
                                }
                                WorkerCommand::Resume { drift_id, result } => {
                                    handle_resume(drift_id, result, &mut rt);
                                }
                            },
                            Err(_) => break,
                        }
                    }
                })
                .expect("Failed to spawn worker");

            workers.push(handle);
        }

        Self {
            request_txs: final_txs,
            round_robin_counter: AtomicUsize::new(0),
            num_workers: num_threads,
            _workers: workers,
        }
    }

    /// Execute an action on a worker. Uses round-robin with work-stealing fallback.
    pub async fn execute(
        &self,
        action: String,
        method: String,
        path: String,
        body: Option<Bytes>,
        headers: SmallVec<[(String, String); 8]>,
        params: SmallVec<[(String, String); 4]>,
        query: SmallVec<[(String, String); 4]>,
    ) -> Result<(serde_json::Value, Vec<(String, f64)>), String> {
        let (tx, rx) = oneshot::channel();
        let task = RequestTask {
            action_name: action,
            body,
            method,
            path,
            headers,
            params,
            query,
            response_tx: tx,
        };

        // Work-Stealing Distribution
        let start_idx = self.round_robin_counter.fetch_add(1, Ordering::Relaxed) % self.num_workers;
        let mut cmd = WorkerCommand::Request(task);

        for attempt in 0..self.num_workers {
            let idx = (start_idx + attempt) % self.num_workers;
            match self.request_txs[idx].try_send(cmd) {
                Ok(()) => {
                    return match rx.await {
                        Ok(res) => Ok((res.json, res.timings)),
                        Err(_) => Err("Worker channel closed".to_string()),
                    };
                }
                Err(TrySendError::Full(returned)) => {
                    cmd = returned;
                }
                Err(TrySendError::Disconnected(_)) => {
                    return Err("Worker disconnected".to_string());
                }
            }
        }

        // All workers full — blocking send to the original target as last resort
        self.request_txs[start_idx]
            .send(cmd)
            .map_err(|e| e.to_string())?;

        match rx.await {
            Ok(res) => Ok((res.json, res.timings)),
            Err(_) => Err("Worker channel closed".to_string()),
        }
    }
}

/// Handle a new incoming request.
///
/// OPTIMIZATION: Deferred cloning.
/// Only stores data if drift (async suspend) happens.
fn handle_new_request(task: RequestTask, rt: &mut TitanRuntime) {
    rt.request_counter += 1;
    let request_id = rt.request_counter;

    // Move response_tx into pending (partial move of task — other fields remain accessible)
    rt.pending_requests.insert(request_id, task.response_tx);

    let drift_count = rt.drift_counter;
    rt.request_start_counters.insert(request_id, drift_count);

    // Execute action — pass references, body is O(1) Bytes clone
    extensions::execute_action_optimized(
        rt,
        request_id,
        &task.action_name,
        task.body.clone(), // Bytes::clone() is O(1) refcount bump
        &task.method,
        &task.path,
        &task.headers,
        &task.params,
        &task.query,
    );

    // Deferred cloning decision
    if !rt.pending_requests.contains_key(&request_id) {
        // Completed synchronously — no data needed, minimal cleanup
        rt.request_start_counters.remove(&request_id);
    } else {
        // Suspended via drift — MOVE (not clone) data for resume replay.
        rt.active_requests.insert(
            request_id,
            extensions::RequestData {
                action_name: task.action_name,
                body: task.body,
                method: task.method,
                path: task.path,
                headers: task.headers.into_vec(),
                params: task.params.into_vec(),
                query: task.query.into_vec(),
            },
        );
    }
}

fn handle_resume(drift_id: u32, result: WorkerAsyncResult, rt: &mut TitanRuntime) {
    let req_id = rt.drift_to_request.get(&drift_id).copied().unwrap_or(0);

    let timing_type = if result.result.get("error").is_some() {
        "drift_error"
    } else {
        "drift"
    };
    rt.request_timings
        .entry(req_id)
        .or_default()
        .push((timing_type.to_string(), result.duration_ms));

    rt.completed_drifts.insert(drift_id, result.result);

    if let Some(req_data) = rt.active_requests.get(&req_id).cloned() {
        let start_counter = rt.request_start_counters.get(&req_id).copied().unwrap_or(0);
        rt.drift_counter = start_counter;

        extensions::execute_action_optimized(
            rt,
            req_id,
            &req_data.action_name,
            req_data.body,
            &req_data.method,
            &req_data.path,
            &req_data.headers,
            &req_data.params,
            &req_data.query,
        );
    }

    if req_id != 0 && !rt.pending_requests.contains_key(&req_id) {
        rt.active_requests.remove(&req_id);
        rt.request_start_counters.remove(&req_id);
    }
}